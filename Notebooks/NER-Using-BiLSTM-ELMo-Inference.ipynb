{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Inference from saved model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_index  ={'I-gpe': 0,\n",
    "            'B-art': 1,\n",
    "            'B-org': 2,\n",
    "            'I-org': 3,\n",
    "            'I-per': 4,\n",
    "            'I-tim': 5,\n",
    "            'I-nat': 6,\n",
    "            'I-geo': 7,\n",
    "            'B-nat': 8,\n",
    "            'B-per': 9,\n",
    "            'B-geo': 10,\n",
    "            'I-eve': 11,\n",
    "            'B-tim': 12,\n",
    "            'I-art': 13,\n",
    "            'B-gpe': 14,\n",
    "            'B-eve': 15,\n",
    "            'O': 16}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sentence_for_inference(sent):\n",
    "\n",
    "    tokenized_words = sent.split(' ')\n",
    "    max_len = 50\n",
    "    words_sentence_padded = []\n",
    "    for i in range(max_len):\n",
    "        try:\n",
    "            # append word on the sentence\n",
    "            words_sentence_padded.append(tokenized_words[i])\n",
    "        except:\n",
    "            # If sequence is less than 50 tokens, add padding manually\n",
    "            words_sentence_padded.append('__PAD__')\n",
    "   \n",
    "    return words_sentence_padded   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Subha\\.conda\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\nInstructions for updating:\nnon-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow.compat.v1 as tf1\n",
    "tf1.disable_eager_execution()\n",
    "tf1.disable_v2_behavior()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using Device: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(f'Using Device: {tf.config.list_physical_devices(\"GPU\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "elmo = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True)\n",
    "batch_size = 1\n",
    "max_len = 50\n",
    "\n",
    "from cachetools import TTLCache,cached\n",
    "cache = TTLCache(maxsize=100, ttl=21600)\n",
    "\n",
    "# @cached(cache)\n",
    "# def ElmoEmbedding(x):\n",
    "#     return elmo(tf.reshape(tf.cast(x,tf.string),[-1]),signature='default',as_dict=True)['elmo']\n",
    "@cached(cache)\n",
    "def ElmoEmbedding(x):\n",
    "    return elmo(inputs={\n",
    "                            \"tokens\": tf.squeeze(tf.cast(x, tf.string)),\n",
    "                            \"sequence_len\": tf.constant(batch_size*[max_len])\n",
    "                      },\n",
    "                      signature=\"tokens\",\n",
    "                      as_dict=True)[\"elmo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model Architecture\n",
    "\n",
    "# input text is a sequence of 50 tokens\n",
    "def get_model(max_len=50):\n",
    "    input_text = tf.keras.layers.Input(shape=(max_len,), dtype='string',name='input_text')\n",
    "    embedding = tf.keras.layers.Lambda(ElmoEmbedding,output_shape = (None,max_len,1024), name = 'elmo_embedding')(input_text)\n",
    "\n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(512,return_sequences=True,recurrent_dropout=0.2,dropout=0.2))(embedding)\n",
    "    x_rnn = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(512,return_sequences=True,recurrent_dropout=0.2,dropout=0.2))(x)\n",
    "\n",
    "    #   residual connection\n",
    "    x = tf.keras.layers.Add()([x,x_rnn])\n",
    "\n",
    "    out = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(17,activation='softmax'))(x)\n",
    "\n",
    "    model =tf.keras.models.Model(input_text,out)\n",
    "    model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "InvalidArgumentError",
     "evalue": "2 root error(s) found.\n  (0) Invalid argument: slice index 1 of dimension 0 out of bounds.\n\t [[{{node elmo_embedding_3/module_2_apply_tokens/strided_slice_1}}]]\n\t [[time_distributed_3/Reshape_1/_491]]\n  (1) Invalid argument: slice index 1 of dimension 0 out of bounds.\n\t [[{{node elmo_embedding_3/module_2_apply_tokens/strided_slice_1}}]]\n0 successful operations.\n0 derived errors ignored.",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-4355c24d1420>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mX_tr_arr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mX_tr_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_tr_arr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mtest_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_tr_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\.conda\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    981\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    982\u001b[0m     \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_select_training_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 983\u001b[1;33m     return func.predict(\n\u001b[0m\u001b[0;32m    984\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, model, x, batch_size, verbose, steps, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    706\u001b[0m     x, _, _ = model._standardize_user_data(\n\u001b[0;32m    707\u001b[0m         x, check_steps=True, steps_name='steps', steps=steps)\n\u001b[1;32m--> 708\u001b[1;33m     return predict_loop(\n\u001b[0m\u001b[0;32m    709\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m         \u001b[1;31m# Get outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 386\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    387\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3822\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_arrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_symbols\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3823\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3824\u001b[1;33m     fetched = self._callable_fn(*array_vals,\n\u001b[0m\u001b[0;32m   3825\u001b[0m                                 run_metadata=self.run_metadata)\n\u001b[0;32m   3826\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1468\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1469\u001b[0m         \u001b[0mrun_metadata_ptr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_NewBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1470\u001b[1;33m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0m\u001b[0;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1472\u001b[0m                                                run_metadata_ptr)\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: 2 root error(s) found.\n  (0) Invalid argument: slice index 1 of dimension 0 out of bounds.\n\t [[{{node elmo_embedding_3/module_2_apply_tokens/strided_slice_1}}]]\n\t [[time_distributed_3/Reshape_1/_491]]\n  (1) Invalid argument: slice index 1 of dimension 0 out of bounds.\n\t [[{{node elmo_embedding_3/module_2_apply_tokens/strided_slice_1}}]]\n0 successful operations.\n0 derived errors ignored."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    sess.run(tf.compat.v1.global_variables_initializer())\n",
    "    sess.run(tf.compat.v1.tables_initializer())\n",
    "    model = get_model()\n",
    "    model.load_weights('../Models/NER_ELMo_biLSTM.h5')\n",
    "    text = convert_sentence_for_inference('Joe Biden, the new president elect of the United States will take the office on 9th November')\n",
    "    X_tr_arr = np.array(text)\n",
    "    X_tr_t = X_tr_arr.reshape(1,50)\n",
    "    test_pred = model.predict(X_tr_t, verbose=1,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " text = convert_sentence_for_inference('Joe Biden, the new president elect of the United States will take the office on 9th November')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = np.array(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([['Joe', 'Biden,', 'the', 'new', 'president', 'elect', 'of', 'the',\n",
       "        'United', 'States', 'will', 'take', 'the', 'office', 'on', '9th',\n",
       "        'November', '__PAD__', '__PAD__', '__PAD__', '__PAD__',\n",
       "        '__PAD__', '__PAD__', '__PAD__', '__PAD__', '__PAD__', '__PAD__',\n",
       "        '__PAD__', '__PAD__', '__PAD__', '__PAD__', '__PAD__', '__PAD__',\n",
       "        '__PAD__', '__PAD__', '__PAD__', '__PAD__', '__PAD__', '__PAD__',\n",
       "        '__PAD__', '__PAD__', '__PAD__', '__PAD__', '__PAD__', '__PAD__',\n",
       "        '__PAD__', '__PAD__', '__PAD__', '__PAD__', '__PAD__']],\n",
       "      dtype='<U9')"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "text.reshape(1,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}